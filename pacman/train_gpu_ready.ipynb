{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "14in29TlVQAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y56tkQupVy9P",
        "outputId": "7871ec16-251a-431d-e111-1678a6829837"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.1.1)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Uh5YgfKjUEl0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base.py"
      ],
      "metadata": {
        "id": "fN8BFNBxVN0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "\tdef __init__(self, device, input_frame_dim, no_frames):\n",
        "\t\tsuper(Actor, self).__init__()\n",
        "\n",
        "\t\t# In this implementation the previous frames and the different rgb channels are in the same dimension\n",
        "\t\t# Reasoning being that on all channels the same kernels will work, and their wouldn't be a need to learn\n",
        "\t\t# different kernels for each frame\n",
        "\n",
        "\t\t# (250,160,3) or 210?\n",
        "\t\tself.conv1 = nn.Conv2d(3*no_frames,  3*2*no_frames, (3,3), groups=1, stride=(2,1))\n",
        "\t\t# (124, 158, 6) of 121 156\n",
        "\t\tself.conv2 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (61, 78, 6) of 63 74\n",
        "\t\tself.conv3 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (30, 38, 6) why this is 30 and not 29 idk\n",
        "\t\tself.conv4 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (14, 18, 6)\n",
        "\t\tself.conv5 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (6, 8, 6)\n",
        "\t\tself.flat = nn.Flatten()\n",
        "\t\t# 6*8*6 = 420\n",
        "\t\tself.full = nn.Linear(5*8*3*2*no_frames, 5)\n",
        "\t\t# 5\n",
        "\n",
        "\t\tself.device = device\n",
        "\t\tself.input_frame_dim = input_frame_dim\n",
        "\t\tself.no_frames = no_frames\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv2(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv3(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv4(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv5(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.flat(x)\n",
        "\n",
        "\t\tx = self.full(x)\n",
        "\t\tx = F.softmax(x, dim=1)\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "\tdef act(self, state_3):\n",
        "\n",
        "\t\t# we bring the last three frames to the model\n",
        "\t\t# that way the model can see how the ghosts move :)\n",
        "\t\t# Only the current frame should be attached\n",
        "\n",
        "\t\tprobabilities = self.forward(state_3)\n",
        "\t\tprobs = Categorical(probabilities)\n",
        "\t\taction = probs.sample()\n",
        "\t\t# PPO paper says to use normal prob here\n",
        "\t\treturn action.item(), probs.log_prob(action).exp()\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "\tdef __init__(self, device, input_frame_dim, no_frames):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\n",
        "\t\t# (250,160,)\n",
        "\t\tself.conv1 = nn.Conv2d(3*no_frames,  3*2*no_frames, (3,3), groups=1, stride=(2,1))\n",
        "\t\t# (124, 158, ...) of 121 156\n",
        "\t\tself.conv2 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (61, 78, 6) of 63 74\n",
        "\t\tself.conv3 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (30, 38, 6) why this is 30 and not 29 idk\n",
        "\t\tself.conv4 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (14, 18, 6)\n",
        "\t\tself.conv5 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (6, 8, 6)\n",
        "\t\tself.flat = nn.Flatten()\n",
        "\t\t# 6*8*6 = 420\n",
        "\t\tself.full = nn.Linear(5*8*3*2*no_frames, 1)\n",
        "\t\t# 1\n",
        "\n",
        "\t\tself.device = device\n",
        "\t\tself.input_frame_dim = input_frame_dim\n",
        "\t\tself.no_frames = no_frames\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv2(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv3(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv4(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv5(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.flat(x)\n",
        "\n",
        "\t\tx = self.full(x)\n",
        "\t\tx = F.sigmoid(x)\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "def state_to_normalized_tensor(state, device):\n",
        "\n",
        "\tstate = np.array(state) / 255\n",
        "\n",
        "\t# PyTorch wants the rgb channel first\n",
        "\ttransposed_array = np.transpose(state, (2, 0, 1))\n",
        "\n",
        "\treturn torch.from_numpy(transposed_array).float().to(device)\n",
        "\n",
        "\n",
        "def collect_episode(env, model):\n",
        "\n",
        "\tepisode = []\n",
        "\tterminated = False\n",
        "\ttruncated = False\n",
        "\n",
        "\t# (rgb, x, y) -> (no_frames*rgb, x, y)\n",
        "\tdim = (model.input_frame_dim[0]*(model.no_frames-1),) + model.input_frame_dim[1:]\n",
        "\n",
        "\tzeros = torch.from_numpy(np.zeros(dim))\n",
        "\n",
        "\tstate_repr = zeros.float().to(model.device)\n",
        "\n",
        "\tnew_state, info = env.reset()\n",
        "\n",
        "\tlast_life_value = info['lives']\n",
        "\n",
        "\twhile not (terminated or truncated):\n",
        "\n",
        "\t\tnew_state = state_to_normalized_tensor(new_state, model.device)\n",
        "\t\tstate_repr = torch.cat((state_repr, new_state))\n",
        "\t\tmodel_ready_state = state_repr.unsqueeze(0)\n",
        "\n",
        "\t\taction, probs = model.act(model_ready_state)\n",
        "\n",
        "\t\tnew_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "\n",
        "\t\tif last_life_value > info['lives']:\n",
        "\t\t\tlast_life_value = info['lives']\n",
        "\t\t\t# in a previous implementation I always summed the number of current lives to the reward\n",
        "\t\t\t# this resulted in pacman hiding in a corner, as staying alive longer -> more rewards\n",
        "\t\t\t# now we just give a penalty when he dies\n",
        "\t\t\treward -= 100\n",
        "\n",
        "\t\t# in this current implementation we save each frame no_frames times\n",
        "\t\t# this is a major driver of memory usage\n",
        "\t\t# this can of course be improved, it would require some more serious refactoring however\n",
        "\t\tepisode.append((model_ready_state, action, probs, reward))\n",
        "\n",
        "\t\t# 3 because we work with rgb, could also polished by integrating into hyperpars\n",
        "\t\t# we pop the last frame here\n",
        "\t\tstate_repr = state_repr[3:]\n",
        "\n",
        "\treturn episode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or-Y9X4GVHyu",
        "outputId": "153982d8-0bbf-434f-a9ab-10dccdd0ff7a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train.py"
      ],
      "metadata": {
        "id": "_QVI6THrVb_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index probability of action taken at sampling time with current updated model\n",
        "def get_probs(actor, states, actions):\n",
        "\tall_probs = torch.cat([actor.forward(state) for state in states])\n",
        "\tactions = torch.Tensor(actions).int()\n",
        "\tout = all_probs[torch.arange(all_probs.size(0)), actions]\n",
        "\t# these lines do this below more efficient\n",
        "\t#[all_probs[i][actions[i]].unsqueeze(0) for i in range(len(actions))]\n",
        "\treturn out\n",
        "\n",
        "\n",
        "def update_net(gradients, optim):\n",
        "\n",
        "\tpolicy_loss = gradients.sum()\n",
        "\toptim.zero_grad()\n",
        "\tpolicy_loss.backward()\n",
        "\toptim.step()\n",
        "\n",
        "\treturn policy_loss\n",
        "\n",
        "\n",
        "def get_standardized_tensor(xs):\n",
        "\n",
        "\t## eps is the smallest representable float, which is\n",
        "\t# added to the standard deviation of the returns to avoid numerical instabilities\n",
        "\teps = np.finfo(np.float32).eps.item()\n",
        "\txs = torch.tensor(xs)\n",
        "\treturn (xs - xs.mean()) / (xs.std() + eps)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: make sure the amount of context frames can be a variable\n",
        "# (such that a state consists of the last x frames)\n",
        "def train(env, actor, critic, optim, num_episodes, num_actors, num_epochs, eps, gamma):\n",
        "\n",
        "\t# One style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\n",
        "\t# with recurrent neural networks, runs the policy for T timesteps (where T is much less than the\n",
        "\t# episode length), and uses the collected samples for an update. This style requires an advantage\n",
        "\t# estimator that does not look beyond timestep T .\n",
        "\t# Don't we already have this? Think about this. Or isn't that the entire idea of the estimator?\n",
        "\n",
        "\tassert actor.device == critic.device\n",
        "\tdevice = actor.device\n",
        "\n",
        "\tobj_func_hist = []\n",
        "\tlosses = []\n",
        "\n",
        "\ttime_sampling = 0\n",
        "\ttime_updating = 0\n",
        "\n",
        "\tfor _ in tqdm(range(num_episodes)):\n",
        "\n",
        "\t\tt0 = time.time()\n",
        "\n",
        "\t\tepisodes = [collect_episode(env, actor) for _ in range(num_actors)]\n",
        "\n",
        "\t\tsum_tuples = 0\n",
        "\t\tfor episode in episodes:\n",
        "\t\t\tsum_tuples += len(episode)\n",
        "\n",
        "\t\t'''\n",
        "\t\tIn other implementations the data is not a collection of varying length concluded episodes,\n",
        "\t\tbut instead an array of i*j, where j is a fixed number of steps.\n",
        "\t\tSo a row could be a partial episode (most likely), or multiple entire episodes,\n",
        "\t\twith possibly the last one being truncated.\n",
        "\t\t'''\n",
        "\n",
        "\t\tstates = [[state for state, _, _, _ in episode] for episode in episodes]\n",
        "\t\tactions = [[action for _, action, _, _ in episode] for episode in episodes]\n",
        "\t\toriginal_probs  = [torch.cat([prob for _, _, prob, _ in episode]) for episode in episodes]\n",
        "\t\trewards = [[reward for _, _, _, reward in episode] for episode in episodes]\n",
        "\n",
        "\t\tcum_rewards = np.array([sum(episode) for episode in rewards])\n",
        "\t\tobj_func_hist.append((cum_rewards.mean(), cum_rewards.min(), cum_rewards.max()))\n",
        "\n",
        "\t\trewards = [get_standardized_tensor(reward).unsqueeze(1).to(device) for reward in rewards]\n",
        "\n",
        "\t\tt1 = time.time()\n",
        "\n",
        "\t\ttime_sampling += t1 - t0\n",
        "\n",
        "\t\tt0 = time.time()\n",
        "\t\tfor k in range(num_epochs):\n",
        "\n",
        "\t\t\tactor_gradient = torch.empty(0).to(device)\n",
        "\t\t\tcritic_gradient = torch.empty(0).to(device)\n",
        "\n",
        "\t\t\tfor j in range(num_actors):\n",
        "\n",
        "\t\t\t\tcritic_values_t  = torch.cat([critic(state) for state in states[j]])\n",
        "\t\t\t\tcritic_values_t1 = torch.cat((critic_values_t.clone()[1:], torch.zeros(1,1).to(device)))\n",
        "\n",
        "\t\t\t\tadvantage = gamma*critic_values_t1 + rewards[j] - critic_values_t\n",
        "\n",
        "\t\t\t\t# we need the probability for each action\n",
        "\t\t\t\tif k == 0:\n",
        "\t\t\t\t\tactor_probs = original_probs[j].clone()\n",
        "\t\t\t\t\toriginal_probs[j] = original_probs[j].detach()\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# we can also always do this and detach already all above\n",
        "\t\t\t\t\tactor_probs = get_probs(actor, states[j], actions[j])\n",
        "\n",
        "\t\t\t\tdifference_grad = torch.div(actor_probs, original_probs[j]).unsqueeze(1)\n",
        "\t\t\t\t# Note that for k = 0 these are all ones, but we keep the calculation such that the\n",
        "\t\t\t\t# backtracking algorithm can also see this\n",
        "\n",
        "\t\t\t\tclipped = torch.clamp(difference_grad, 1 - eps, 1 + eps)\n",
        "\n",
        "\t\t\t\tppo_gradient = torch.minimum(difference_grad*advantage, clipped*advantage)\n",
        "\t\t\t\tppo_gradient *= -1 # this seems to be the right place\n",
        "\n",
        "\t\t\t\tactor_gradient = torch.cat((actor_gradient, ppo_gradient))\n",
        "\t\t\t\t# we could also include an \"entropy\" bonus to the actor loss that encourages exploration\n",
        "\n",
        "\t\t\t\tcritic_gradient = torch.cat((critic_gradient, advantage*advantage))\n",
        "\n",
        "\t\t\t# update both models\n",
        "\t\t\tgradient = torch.cat((actor_gradient, critic_gradient))\n",
        "\t\t\tloss = update_net(gradient, optim)\n",
        "\t\t\tlosses.append(loss.detach().cpu())\n",
        "\n",
        "\t\tt1 = time.time()\n",
        "\n",
        "\t\ttime_updating += t1 - t0\n",
        "\n",
        "\tprint(f\"Total time sampling = {time_sampling}\")\n",
        "\tprint(f\"Total time updating = {time_updating}\")\n",
        "\n",
        "\treturn obj_func_hist, losses\n",
        "\n",
        "\n",
        "def plot_fancy_loss(data, path):\n",
        "\n",
        "\tmeans, mins, maxes = list(map(list, zip(*data)))\n",
        "\n",
        "\tplt.clf()\n",
        "\n",
        "\txs = range(len(data))\n",
        "\n",
        "\t# Plot points\n",
        "\tplt.plot(xs, means, 'b', alpha=0.8)\n",
        "\t#plt.plot(pfb_x_mean, pfb_y_mean, 'g', alpha=0.8)\n",
        "\n",
        "\t# Plot errors\n",
        "\tplt.fill_between(xs, mins, maxes, color='b', alpha=0.3)\n",
        "\t#plt.fill_between(pfb_x_mean, pfb_y_low, pfb_y_high, color='g', alpha=0.3)\n",
        "\n",
        "\t# Set labels\n",
        "\tplt.title(f'Reward projection')\n",
        "\tplt.xlabel('Episode No.')\n",
        "\tplt.ylabel(f'Reward')\n",
        "\tplt.legend(['My PPO implementation'])#, 'PPO for Beginners'])\\\n",
        "\tplt.savefig(path)\n",
        "\n",
        "\n",
        "def plot_ugly_loss(data, length, name):\n",
        "\n",
        "\tplt.clf()\n",
        "\tplt.plot(list(range(length)), data)\n",
        "\tplt.savefig(f\"{base_path}/{name}loss.png\")\n",
        "\n",
        "\n",
        "# 3 HOUR TRAINING TIME?????\n",
        "pacman_hyperparameters = {\n",
        "\t\"num_episodes\" : 300,\n",
        "\t\"gamma\" : 0.99,\n",
        "\t\"lr\" : 1e-3,\n",
        "\t\"env_name\" : \"ALE/MsPacman-v5\",\n",
        "\t\"frameskip\" : 4,\n",
        "\t\"repeat_action_probability\": 0.2,\n",
        "\t\"render_mode\" : \"rgb_array\",\n",
        "\t\"trial_number\" : 4,\n",
        "\t\"eps\" : 0.2,\n",
        "\t\"num_epochs\" : 10,\n",
        "\t\"num_actors\" : 3,\n",
        "\t\"device\" : \"cuda\",\n",
        "\t\"obs_type\" : \"rgb\",\n",
        "\t\"input_frame_dim\" : (3,210,160),\n",
        "\t\"no_frames\" : 5 # how many frames the model can look back (all frames given to model input)\n",
        "}\n",
        "# next trial: no_frames 3 -> 5, num_episodes 100 -> 300, numb_epochs 5 -> 10\n",
        "# should take about 4 hrs\n",
        "\n",
        "# input (250, 160, 3)\n",
        "# reward float, increases when pacman eats. Further information about reward func unknown\n",
        "# info contains lives, dict object\n",
        "\n",
        "env = gym.make(pacman_hyperparameters[\"env_name\"],\n",
        "\t\t\t   render_mode=pacman_hyperparameters[\"render_mode\"]\n",
        "\t\t\t   )\n",
        "\n",
        "base_path = f\"trial_data/trial_{pacman_hyperparameters['trial_number']}\"\n",
        "\n",
        "env = gym.wrappers.RecordVideo(env, f\"{base_path}/video/\", episode_trigger=lambda t: t % 100 == 99)\n",
        "\n",
        "actor = Actor(pacman_hyperparameters[\"device\"],\n",
        "\t\t\t  pacman_hyperparameters[\"input_frame_dim\"],\n",
        "\t\t\t  pacman_hyperparameters[\"no_frames\"])\n",
        "actor.to(actor.device)\n",
        "\n",
        "critic = Critic(pacman_hyperparameters[\"device\"],\n",
        "\t\t\t\tpacman_hyperparameters[\"input_frame_dim\"],\n",
        "\t\t\t\tpacman_hyperparameters[\"no_frames\"])\n",
        "critic.to(critic.device)\n",
        "\n",
        "optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=pacman_hyperparameters[\"lr\"])\n",
        "\n",
        "obj_func_hist, losses = train(env,\n",
        "\t\t\t\t\t\t\t  actor,\n",
        "\t\t\t\t\t\t\t  critic,\n",
        "\t\t\t\t\t\t\t  optimizer,\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"num_episodes\"],\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"num_actors\"],\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"num_epochs\"],\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"eps\"],\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"gamma\"])\n",
        "\n",
        "json.dump(pacman_hyperparameters, open(f\"{base_path}/hyperparameters.json\",'w'))\n",
        "\n",
        "\n",
        "# check if directory exist, if not, make it\n",
        "Path(f\"{base_path}/save/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "plot_fancy_loss(obj_func_hist,\n",
        "\t\t\t\tf\"{base_path}/rewards.png\")\n",
        "\n",
        "plot_ugly_loss(losses,\n",
        "\t\t\t   len(losses),\n",
        "\t\t\t   \"total_\")\n",
        "\n",
        "torch.save(actor.state_dict(), f\"{base_path}/save/actor_weights.pt\")\n",
        "torch.save(critic.state_dict(), f\"{base_path}/save/critic_weights.pt\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9g5hGzGVa14",
        "outputId": "42890dd9-faa3-47ea-f4ae-317c7b3c00cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|‚ñè         | 6/300 [05:18<4:34:00, 55.92s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW_cOCR7X2JG",
        "outputId": "da453e58-22cc-4315-e869-6e1c6b1c5763"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    }
  ]
}