{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "14in29TlVQAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y56tkQupVy9P",
        "outputId": "695ef912-8a0e-4bc5-ab20-dc0ec0bcb90e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[atari]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.1.1)\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy\n",
            "Successfully installed ale-py-0.8.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-0.2.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2023.11.17)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=5c79b93002edcba0a54d73c32ad27143c41d7810f6f7912751d6908094498517\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Uh5YgfKjUEl0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base.py"
      ],
      "metadata": {
        "id": "fN8BFNBxVN0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "\tdef __init__(self, device, input_frame_dim, no_frames):\n",
        "\t\tsuper(Actor, self).__init__()\n",
        "\n",
        "\t\t# In this implementation the previous frames and the different rgb channels are in the same dimension\n",
        "\t\t# Reasoning being that on all channels the same kernels will work, and their wouldn't be a need to learn\n",
        "\t\t# different kernels for each frame\n",
        "\n",
        "\t\t# (250,160,3) or 210?\n",
        "\t\tself.conv1 = nn.Conv2d(3*no_frames,  3*2*no_frames, (3,3), groups=1, stride=(2,1))\n",
        "\t\t# (124, 158, 6) of 121 156\n",
        "\t\tself.conv2 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (61, 78, 6) of 63 74\n",
        "\t\tself.conv3 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (30, 38, 6) why this is 30 and not 29 idk\n",
        "\t\tself.conv4 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (14, 18, 6)\n",
        "\t\tself.conv5 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (6, 8, 6)\n",
        "\t\tself.flat = nn.Flatten()\n",
        "\t\t# 6*8*6 = 420\n",
        "\t\tself.full = nn.Linear(5*8*3*2*no_frames, 5)\n",
        "\t\t# 5\n",
        "\n",
        "\t\tself.device = device\n",
        "\t\tself.input_frame_dim = input_frame_dim\n",
        "\t\tself.no_frames = no_frames\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv2(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv3(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv4(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv5(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.flat(x)\n",
        "\n",
        "\t\tx = self.full(x)\n",
        "\t\tx = F.softmax(x, dim=1)\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "\tdef act(self, state_3):\n",
        "\n",
        "\t\t# we bring the last three frames to the model\n",
        "\t\t# that way the model can see how the ghosts move :)\n",
        "\t\t# Only the current frame should be attached\n",
        "\n",
        "\t\tprobabilities = self.forward(state_3)\n",
        "\t\tprobs = Categorical(probabilities)\n",
        "\t\taction = probs.sample()\n",
        "\t\t# PPO paper says to use normal prob here\n",
        "\t\treturn action.item(), probs.log_prob(action).exp()\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "\tdef __init__(self, device, input_frame_dim, no_frames):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\n",
        "\t\t# (250,160,)\n",
        "\t\tself.conv1 = nn.Conv2d(3*no_frames,  3*2*no_frames, (3,3), groups=1, stride=(2,1))\n",
        "\t\t# (124, 158, ...) of 121 156\n",
        "\t\tself.conv2 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (61, 78, 6) of 63 74\n",
        "\t\tself.conv3 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (30, 38, 6) why this is 30 and not 29 idk\n",
        "\t\tself.conv4 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (14, 18, 6)\n",
        "\t\tself.conv5 = nn.Conv2d(3*2*no_frames, 3*2*no_frames, (3,3), groups=1, stride=(2,2))\n",
        "\t\t# (6, 8, 6)\n",
        "\t\tself.flat = nn.Flatten()\n",
        "\t\t# 6*8*6 = 420\n",
        "\t\tself.full = nn.Linear(5*8*3*2*no_frames, 1)\n",
        "\t\t# 1\n",
        "\n",
        "\t\tself.device = device\n",
        "\t\tself.input_frame_dim = input_frame_dim\n",
        "\t\tself.no_frames = no_frames\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv2(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv3(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv4(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.conv5(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.flat(x)\n",
        "\n",
        "\t\tx = self.full(x)\n",
        "\t\tx = F.sigmoid(x)\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "def state_to_normalized_tensor(state, device):\n",
        "\n",
        "\tstate = np.array(state) / 255\n",
        "\n",
        "\t# PyTorch wants the rgb channel first\n",
        "\ttransposed_array = np.transpose(state, (2, 0, 1))\n",
        "\n",
        "\treturn torch.from_numpy(transposed_array).float().to(device)\n",
        "\n",
        "\n",
        "def collect_episode(env, model):\n",
        "\n",
        "\tepisode = []\n",
        "\tterminated = False\n",
        "\ttruncated = False\n",
        "\n",
        "\t# (rgb, x, y) -> (no_frames*rgb, x, y)\n",
        "\tdim = (model.input_frame_dim[0]*(model.no_frames-1),) + model.input_frame_dim[1:]\n",
        "\n",
        "\tzeros = torch.from_numpy(np.zeros(dim))\n",
        "\n",
        "\tstate_repr = zeros.float().to(model.device)\n",
        "\n",
        "\tnew_state, info = env.reset()\n",
        "\n",
        "\tlast_life_value = info['lives']\n",
        "\n",
        "\twhile not (terminated or truncated):\n",
        "\n",
        "\t\tframe = state_to_normalized_tensor(new_state, model.device)\n",
        "\t\tstate_repr = torch.cat((state_repr, frame))\n",
        "\t\tmodel_ready_state = state_repr.unsqueeze(0)\n",
        "\n",
        "\t\taction, probs = model.act(model_ready_state)\n",
        "\n",
        "\t\tnew_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "\n",
        "\t\tif last_life_value > info['lives']:\n",
        "\t\t\tlast_life_value = info['lives']\n",
        "\t\t\t# in a previous implementation I always summed the number of current lives to the reward\n",
        "\t\t\t# this resulted in pacman hiding in a corner, as staying alive longer -> more rewards\n",
        "\t\t\t# now we just give a penalty when he dies\n",
        "\t\t\treward -= 100\n",
        "\n",
        "\t\t# For memory reasons we only save the frame within the state\n",
        "\t\t# These can be recovered later, using the functions provided\n",
        "\t\tepisode.append((frame, action, probs, reward))\n",
        "\n",
        "\t\t# 3 because we work with rgb, could also polished by integrating into hyperpars\n",
        "\t\t# we pop the last frame here\n",
        "\t\tstate_repr = state_repr[3:]\n",
        "\n",
        "\treturn episode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or-Y9X4GVHyu",
        "outputId": "d5e84771-bf17-4492-fe38-a952b50b23e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train.py"
      ],
      "metadata": {
        "id": "_QVI6THrVb_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index probability of action taken at sampling time with current updated model\n",
        "def get_probs(actor, frames, actions):\n",
        "\t#all_probs = torch.cat([actor.forward(state) for state in states])\n",
        "\tall_probs = torch.cat([actor.forward(get_state(i, frames, actor)) for i in range(len(frames))])\n",
        "\tactions = torch.Tensor(actions).int()\n",
        "\tout = all_probs[torch.arange(all_probs.size(0)), actions]\n",
        "\t# these lines do this below more efficient\n",
        "\t#[all_probs[i][actions[i]].unsqueeze(0) for i in range(len(actions))]\n",
        "\treturn out\n",
        "\n",
        "\n",
        "def get_standardized_tensor(xs):\n",
        "\n",
        "\t## eps is the smallest representable float, which is\n",
        "\t# added to the standard deviation of the returns to avoid numerical instabilities\n",
        "\teps = np.finfo(np.float32).eps.item()\n",
        "\txs = torch.tensor(xs)\n",
        "\treturn (xs - xs.mean()) / (xs.std() + eps)\n",
        "\n",
        "\n",
        "# from the array of frames this function constructs the state at time step i\n",
        "def get_state(i, frames, model):\n",
        "\n",
        "\tdevice = model.device\n",
        "\tno_frames = model.no_frames\n",
        "\tframe_dim = model.input_frame_dim\n",
        "\n",
        "\tno_empty_frames = no_frames - i - 1 # i = 0 & no_frames = 3 -> = 2\n",
        "\tleft_idx = max(0, i - no_frames + 1)# i = 0 & no_frames = 3 -> = 0\n",
        "\tright_idx = i + 1\n",
        "\n",
        "\tframes_tensor = torch.cat((frames[left_idx:right_idx]))\n",
        "\n",
        "\tif no_empty_frames > 0:\n",
        "\t\tzeros = torch.zeros(3*no_empty_frames, frame_dim[1], frame_dim[2]).to(device)\n",
        "\t\tframes_tensor = torch.cat((zeros, frames_tensor))\n",
        "\n",
        "\treturn frames_tensor.unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: make sure the amount of context frames can be a variable\n",
        "# (such that a state consists of the last x frames)\n",
        "def train(env, actor, critic, optim, num_episodes, num_actors, num_epochs, eps, gamma):\n",
        "\n",
        "\t# One style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\n",
        "\t# with recurrent neural networks, runs the policy for T timesteps (where T is much less than the\n",
        "\t# episode length), and uses the collected samples for an update. This style requires an advantage\n",
        "\t# estimator that does not look beyond timestep T .\n",
        "\t# Don't we already have this? Think about this. Or isn't that the entire idea of the estimator?\n",
        "\n",
        "\tassert actor.device == critic.device\n",
        "\tassert actor.no_frames == critic.no_frames\n",
        "\tassert actor.input_frame_dim == critic.input_frame_dim\n",
        "\n",
        "\tdevice = actor.device\n",
        "\n",
        "\tobj_func_hist = []\n",
        "\tlosses = []\n",
        "\n",
        "\ttime_sampling = 0\n",
        "\ttime_updating = 0\n",
        "\n",
        "\tfor _ in tqdm(range(num_episodes)):\n",
        "\n",
        "\t\tt0 = time.time()\n",
        "\n",
        "\t\tepisodes = [collect_episode(env, actor) for _ in range(num_actors)]\n",
        "\n",
        "\n",
        "\t\t'''\n",
        "\t\tIn other implementations the data is not a collection of varying length concluded episodes,\n",
        "\t\tbut instead an array of i*j, where j is a fixed number of steps.\n",
        "\t\tSo a row could be a partial episode (most likely), or multiple entire episodes,\n",
        "\t\twith possibly the last one being truncated.\n",
        "\t\t'''\n",
        "\n",
        "\t\tframes = [[frame for frame, _, _, _ in episode] for episode in episodes]\n",
        "\t\tactions = [[action for _, action, _, _ in episode] for episode in episodes]\n",
        "\t\toriginal_probs  = [torch.cat([prob for _, _, prob, _ in episode]) for episode in episodes]\n",
        "\t\trewards = [[reward for _, _, _, reward in episode] for episode in episodes]\n",
        "\n",
        "\t\tcum_rewards = np.array([sum(episode) for episode in rewards])\n",
        "\t\tobj_func_hist.append((cum_rewards.mean(), cum_rewards.min(), cum_rewards.max()))\n",
        "\n",
        "\t\trewards = [get_standardized_tensor(reward).unsqueeze(1).to(device) for reward in rewards]\n",
        "\n",
        "\t\tt1 = time.time()\n",
        "\n",
        "\t\ttime_sampling += t1 - t0\n",
        "\n",
        "\t\tt0 = time.time()\n",
        "\t\tfor k in range(num_epochs):\n",
        "\n",
        "\t\t\tloss = 0\n",
        "\n",
        "\t\t\tfor j in range(num_actors):\n",
        "\n",
        "\t\t\t\t# 7gb this\n",
        "\t\t\t\t#states = [get_state(i, frames[j], no_frames, frame_dim) for i in range(len(frames[j]))]\n",
        "\n",
        "\t\t\t\t#critic_values_t  = torch.cat([critic(state) for state in states])\n",
        "\t\t\t\tcritic_values_t  = torch.cat([critic(get_state(i, frames[j], critic)) for i in range(len(frames[j]))])\n",
        "\t\t\t\tcritic_values_t1 = torch.cat((critic_values_t[1:], torch.zeros(1,1).to(device)))\n",
        "\n",
        "\t\t\t\tadvantage = gamma*critic_values_t1 + rewards[j] - critic_values_t\n",
        "\n",
        "\t\t\t\t# we need the probability for each action\n",
        "\t\t\t\tif k == 0:\n",
        "\t\t\t\t\tactor_probs = original_probs[j].clone()\n",
        "\t\t\t\t\toriginal_probs[j] = original_probs[j].detach()\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# we can also always do this and detach already all above\n",
        "\t\t\t\t\t#actor_probs = get_probs(actor, states, actions[j])\n",
        "\t\t\t\t\tactor_probs = get_probs(actor, frames[j], actions[j])\n",
        "\n",
        "\t\t\t\tdifference_grad = torch.div(actor_probs, original_probs[j]).unsqueeze(1)\n",
        "\t\t\t\t# Note that for k = 0 these are all ones, but we keep the calculation such that the\n",
        "\t\t\t\t# backtracking algorithm can also see this\n",
        "\n",
        "\t\t\t\tclipped = torch.clamp(difference_grad, 1 - eps, 1 + eps)\n",
        "\n",
        "\t\t\t\tppo_gradient = torch.minimum(difference_grad*advantage, clipped*advantage)\n",
        "\t\t\t\tppo_gradient *= -1 # this seems to be the right place\n",
        "\n",
        "\t\t\t\tloss += ppo_gradient.sum() + (advantage**2).sum()\n",
        "\t\t\t\t# we could also include an \"entropy\" bonus to the actor loss that encourages exploration\n",
        "\n",
        "\t\t\t# update both models\n",
        "\t\t\toptim.zero_grad()\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptim.step()\n",
        "\t\t\tlosses.append(loss.detach().cpu())\n",
        "\n",
        "\t\tt1 = time.time()\n",
        "\n",
        "\t\ttime_updating += t1 - t0\n",
        "\n",
        "\tprint(f\"Total time sampling = {time_sampling}\")\n",
        "\tprint(f\"Total time updating = {time_updating}\")\n",
        "\n",
        "\treturn obj_func_hist, losses\n",
        "\n",
        "\n",
        "def plot_fancy_loss(data, path):\n",
        "\n",
        "\tmeans, mins, maxes = list(map(list, zip(*data)))\n",
        "\n",
        "\tplt.clf()\n",
        "\n",
        "\txs = range(len(data))\n",
        "\n",
        "\t# Plot points\n",
        "\tplt.plot(xs, means, 'b', alpha=0.8)\n",
        "\t#plt.plot(pfb_x_mean, pfb_y_mean, 'g', alpha=0.8)\n",
        "\n",
        "\t# Plot errors\n",
        "\tplt.fill_between(xs, mins, maxes, color='b', alpha=0.3)\n",
        "\t#plt.fill_between(pfb_x_mean, pfb_y_low, pfb_y_high, color='g', alpha=0.3)\n",
        "\n",
        "\t# Set labels\n",
        "\tplt.title(f'Reward projection')\n",
        "\tplt.xlabel('Episode No.')\n",
        "\tplt.ylabel(f'Reward')\n",
        "\tplt.legend(['My PPO implementation'])#, 'PPO for Beginners'])\\\n",
        "\tplt.savefig(path)\n",
        "\n",
        "\n",
        "def plot_ugly_loss(data, length, name):\n",
        "\n",
        "\tplt.clf()\n",
        "\tplt.plot(list(range(length)), data)\n",
        "\tplt.savefig(f\"{base_path}/{name}loss.png\")\n",
        "\n",
        "\n",
        "# 3 HOUR TRAINING TIME?????\n",
        "pacman_hyperparameters = {\n",
        "\t\"num_episodes\" : 300,\n",
        "\t\"gamma\" : 0.99,\n",
        "\t\"lr\" : 1e-3,\n",
        "\t\"env_name\" : \"ALE/MsPacman-v5\",\n",
        "\t\"frameskip\" : 4,\n",
        "\t\"repeat_action_probability\": 0.2,\n",
        "\t\"render_mode\" : \"rgb_array\",\n",
        "\t\"trial_number\" : 4,\n",
        "\t\"eps\" : 0.2,\n",
        "\t\"num_epochs\" : 10,\n",
        "\t\"num_actors\" : 3,\n",
        "\t\"device\" : \"cuda\",\n",
        "\t\"obs_type\" : \"rgb\",\n",
        "\t\"input_frame_dim\" : (3,210,160),\n",
        "\t\"no_frames\" : 4 # how many frames the model can look back (all frames given to model input)\n",
        "}\n",
        "# next trial: no_frames 3 -> 5, num_episodes 100 -> 300, numb_epochs 5 -> 10\n",
        "# should take about 4 hrs\n",
        "\n",
        "# input (250, 160, 3)\n",
        "# reward float, increases when pacman eats. Further information about reward func unknown\n",
        "# info contains lives, dict object\n",
        "\n",
        "env = gym.make(pacman_hyperparameters[\"env_name\"],\n",
        "\t\t\t   render_mode=pacman_hyperparameters[\"render_mode\"]\n",
        "\t\t\t   )\n",
        "\n",
        "base_path = f\"trial_data/trial_{pacman_hyperparameters['trial_number']}\"\n",
        "\n",
        "env = gym.wrappers.RecordVideo(env, f\"{base_path}/video/\", episode_trigger=lambda t: t % 100 == 99)\n",
        "\n",
        "actor = Actor(pacman_hyperparameters[\"device\"],\n",
        "\t\t\t  pacman_hyperparameters[\"input_frame_dim\"],\n",
        "\t\t\t  pacman_hyperparameters[\"no_frames\"])\n",
        "actor.to(actor.device)\n",
        "\n",
        "critic = Critic(pacman_hyperparameters[\"device\"],\n",
        "\t\t\t\tpacman_hyperparameters[\"input_frame_dim\"],\n",
        "\t\t\t\tpacman_hyperparameters[\"no_frames\"])\n",
        "critic.to(critic.device)\n",
        "\n",
        "optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=pacman_hyperparameters[\"lr\"])\n",
        "\n",
        "obj_func_hist, losses = train(env,\n",
        "\t\t\t\t\t\t\t  actor,\n",
        "\t\t\t\t\t\t\t  critic,\n",
        "\t\t\t\t\t\t\t  optimizer,\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"num_episodes\"],\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"num_actors\"],\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"num_epochs\"],\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"eps\"],\n",
        "\t\t\t\t\t\t\t  pacman_hyperparameters[\"gamma\"])\n",
        "\n",
        "json.dump(pacman_hyperparameters, open(f\"{base_path}/hyperparameters.json\",'w'))\n",
        "\n",
        "\n",
        "# check if directory exist, if not, make it\n",
        "Path(f\"{base_path}/save/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "plot_fancy_loss(obj_func_hist,\n",
        "\t\t\t\tf\"{base_path}/rewards.png\")\n",
        "\n",
        "plot_ugly_loss(losses,\n",
        "\t\t\t   len(losses),\n",
        "\t\t\t   \"total_\")\n",
        "\n",
        "torch.save(actor.state_dict(), f\"{base_path}/save/actor_weights.pt\")\n",
        "torch.save(critic.state_dict(), f\"{base_path}/save/critic_weights.pt\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "E9g5hGzGVa14",
        "outputId": "7329ce09-c568-415f-9630-61ac2a47eab1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "439\n",
            "469\n",
            "517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/300 [00:57<4:46:01, 57.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "603\n",
            "453\n",
            "625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/300 [01:04<5:19:30, 64.11s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6e435b12855c>\u001b[0m in \u001b[0;36m<cell line: 224>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpacman_hyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m obj_func_hist, losses = train(env,\n\u001b[0m\u001b[1;32m    225\u001b[0m                                                           \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                                                           \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-6e435b12855c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, actor, critic, optim, num_episodes, num_actors, num_epochs, eps, gamma)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                                 \u001b[0;31m#critic_values_t  = torch.cat([critic(state) for state in states])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                                 \u001b[0mcritic_values_t\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                                 \u001b[0mcritic_values_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_values_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-6e435b12855c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                                 \u001b[0;31m#critic_values_t  = torch.cat([critic(state) for state in states])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                                 \u001b[0mcritic_values_t\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                                 \u001b[0mcritic_values_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_values_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-01adfc97c4a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 5.06 MiB is free. Process 3784 has 14.74 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 189.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vW_cOCR7X2JG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}