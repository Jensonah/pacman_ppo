{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVHTV4WnF5wT",
        "outputId": "e1598616-ffca-49b4-99fd-46dbd43277ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post1\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1.post1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373078 sha256=58b3eb6d0f11dd3841dbc0efc9f0485bd8cb240be927c4f1228d9dbdeab4469a\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T13:05:11.701425Z",
          "iopub.execute_input": "2023-12-25T13:05:11.701911Z",
          "iopub.status.idle": "2023-12-25T13:05:15.277118Z",
          "shell.execute_reply.started": "2023-12-25T13:05:11.701871Z",
          "shell.execute_reply": "2023-12-25T13:05:15.275198Z"
        },
        "trusted": true,
        "id": "5KU1aOlfFsM_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "\tdef __init__(self, device):\n",
        "\t\tsuper(Actor, self).__init__()\n",
        "\t\tself.fc1 = nn.Linear(8, 120)\n",
        "\t\tself.fc2 = nn.Linear(120, 240)\n",
        "\t\tself.fc3 = nn.Linear(240, 120)\n",
        "\t\tself.fc4 = nn.Linear(120, 4)\n",
        "\n",
        "\t\tself.device = device\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.fc2(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.fc3(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.fc4(x)\n",
        "\t\tx = F.softmax(x, dim=1)\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "\tdef act(self, state):\n",
        "\n",
        "\t\tprobabilities = self.forward(state)\n",
        "\t\tprobs = Categorical(probabilities)\n",
        "\t\taction = probs.sample()\n",
        "\t\t# TODO: log_prob or normal prob? Paper says normal prob...\n",
        "\t\treturn action.item(), probs.log_prob(action).exp()\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "\tdef __init__(self, device):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\t\tself.fc1 = nn.Linear(8, 120)\n",
        "\t\tself.fc2 = nn.Linear(120, 240)\n",
        "\t\tself.fc3 = nn.Linear(240, 120)\n",
        "\t\tself.fc4 = nn.Linear(120, 1)\n",
        "\n",
        "\t\tself.device = device\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.fc2(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.fc3(x)\n",
        "\t\tx = F.relu(x)\n",
        "\n",
        "\t\tx = self.fc4(x)\n",
        "\t\tx = F.sigmoid(x)\n",
        "\n",
        "\t\treturn x\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T13:05:15.278915Z",
          "iopub.status.idle": "2023-12-25T13:05:15.279507Z",
          "shell.execute_reply.started": "2023-12-25T13:05:15.279217Z",
          "shell.execute_reply": "2023-12-25T13:05:15.279245Z"
        },
        "trusted": true,
        "id": "INBTLqBEFsNB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_state(state):\n",
        "\n",
        "\tstate[0] /= 1.5\n",
        "\tstate[1] /= 1.5\n",
        "\tstate[2] /= 5.0\n",
        "\tstate[3] /= 5.0\n",
        "\tstate[4] /= 3.1415927\n",
        "\tstate[5] /= 5.0\n",
        "\t# idx 6 and 7 are bools\n",
        "\n",
        "\treturn state\n",
        "\n",
        "\n",
        "# index probability of action taken at sampling time with current updated model\n",
        "def get_probs(actor, states, actions):\n",
        "\tall_probs = torch.cat([actor.forward(state) for state in states])\n",
        "\tactions = torch.Tensor(actions).int().to(actor.device)\n",
        "\tout = all_probs[torch.arange(all_probs.size(0)), actions]\n",
        "\t# these lines do this below more efficient\n",
        "\t#[all_probs[i][actions[i]].unsqueeze(0) for i in range(len(actions))]\n",
        "\treturn out\n",
        "\n",
        "\n",
        "def get_standardized_tensor(xs):\n",
        "\n",
        "\t## eps is the smallest representable float, which is\n",
        "\t# added to the standard deviation of the returns to avoid numerical instabilities\n",
        "\teps = np.finfo(np.float32).eps.item()\n",
        "\txs = torch.tensor(xs)\n",
        "\treturn (xs - xs.mean()) / (xs.std() + eps)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T13:05:15.281340Z",
          "iopub.status.idle": "2023-12-25T13:05:15.281898Z",
          "shell.execute_reply.started": "2023-12-25T13:05:15.281623Z",
          "shell.execute_reply": "2023-12-25T13:05:15.281651Z"
        },
        "trusted": true,
        "id": "oUp_lSpNFsND"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_net(gradients, optim):\n",
        "\n",
        "\tpolicy_loss = gradients.sum()\n",
        "\toptim.zero_grad()\n",
        "\tpolicy_loss.backward()\n",
        "\toptim.step()\n",
        "\n",
        "\treturn policy_loss\n",
        "\n",
        "\n",
        "def collect_episode(env, model):\n",
        "\n",
        "\tepisode = []\n",
        "\tterminated = False\n",
        "\ttruncated = False\n",
        "\n",
        "\tnew_state, info = env.reset()\n",
        "\n",
        "\twhile not (terminated or truncated):\n",
        "\n",
        "\t\tstate = normalize_state(new_state)\n",
        "\t\tstate = torch.from_numpy(state).float().unsqueeze(0).to(model.device)\n",
        "\n",
        "\t\taction, log_probs = model.act(state)\n",
        "\n",
        "\t\tnew_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "\t\tepisode.append((state, action, log_probs, reward))\n",
        "\n",
        "\treturn episode\n",
        "\n",
        "\n",
        "def train(env, actor, critic, optim, num_episodes, num_actors, num_epochs, eps, gamma):\n",
        "\n",
        "\t# One style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\n",
        "\t# with recurrent neural networks, runs the policy for T timesteps (where T is much less than the\n",
        "\t# episode length), and uses the collected samples for an update. This style requires an advantage\n",
        "\t# estimator that does not look beyond timestep T .\n",
        "\t# Don't we already have this? Think about this. Or isn't that the entire idea of the estimator?\n",
        "\n",
        "\tassert actor.device == critic.device\n",
        "\tdevice = actor.device\n",
        "\n",
        "\tobj_func_hist = []\n",
        "\tlosses = []\n",
        "\n",
        "\tfor _ in tqdm(range(num_episodes)):\n",
        "\n",
        "\t\tepisodes = [collect_episode(env, actor) for _ in range(num_actors)]\n",
        "\n",
        "\t\tsum_tuples = 0\n",
        "\t\tfor episode in episodes:\n",
        "\t\t\tsum_tuples += len(episode)\n",
        "\n",
        "\t\t'''\n",
        "\t\tIn other implementations the data is not a collection of varying length concluded episodes,\n",
        "\t\tbut instead an array of i*j, where j is a fixed number of steps.\n",
        "\t\tSo a row could be a partial episode (most likely), or multiple entire episodes,\n",
        "\t\twith possibly the last one being truncated.\n",
        "\t\t'''\n",
        "\n",
        "\t\tstates = [[state for state, _, _, _ in episode] for episode in episodes]\n",
        "\t\tactions = [[action for _, action, _, _ in episode] for episode in episodes]\n",
        "\t\toriginal_probs  = [torch.cat([prob for _, _, prob, _ in episode]).to(device) for episode in episodes]\n",
        "\t\trewards = [[reward for _, _, _, reward in episode] for episode in episodes]\n",
        "\n",
        "\t\tcum_rewards = np.array([sum(episode) for episode in rewards])\n",
        "\t\tobj_func_hist.append((cum_rewards.mean(), cum_rewards.min(), cum_rewards.max()))\n",
        "\n",
        "\t\trewards = [get_standardized_tensor(reward).unsqueeze(1).to(device) for reward in rewards]\n",
        "\n",
        "\t\tfor k in range(num_epochs):\n",
        "\n",
        "\t\t\tactor_gradient = torch.empty(0).to(device)\n",
        "\t\t\tcritic_gradient = torch.empty(0).to(device)\n",
        "\n",
        "\t\t\tfor j in range(num_actors):\n",
        "\n",
        "\t\t\t\tcritic_values_t  = torch.cat([critic(state) for state in states[j]]).to(device)\n",
        "\t\t\t\tcritic_values_t1 = torch.cat((critic_values_t.clone()[1:], torch.zeros(1,1).to(device)))\n",
        "\n",
        "\t\t\t\tadvantage = gamma*critic_values_t1 + rewards[j] - critic_values_t\n",
        "\t\t\t\t#advantage *= -1 # flipping because torch minimizes\n",
        "\n",
        "\t\t\t\t# we need the probability for each action\n",
        "\t\t\t\tif k == 0:\n",
        "\t\t\t\t\tactor_probs = original_probs[j].clone()\n",
        "\t\t\t\t\toriginal_probs[j] = original_probs[j].detach()\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# we can also always do this and detach already all above\n",
        "\t\t\t\t\tactor_probs = get_probs(actor, states[j], actions[j])\n",
        "\n",
        "\t\t\t\tdifference_grad = torch.div(actor_probs, original_probs[j]).unsqueeze(1)\n",
        "\t\t\t\t# Note that for k = 0 these are all ones, but we keep the calculation such that the\n",
        "\t\t\t\t# backtracking algorithm can also see this\n",
        "\n",
        "\t\t\t\tclipped = torch.clamp(difference_grad, 1 - eps, 1 + eps)\n",
        "\n",
        "\t\t\t\tppo_gradient = torch.minimum(difference_grad*advantage, clipped*advantage)\n",
        "\t\t\t\tppo_gradient *= -1 # this seems to be the right place\n",
        "\n",
        "\t\t\t\tactor_gradient = torch.cat((actor_gradient, ppo_gradient))\n",
        "\t\t\t\t# we could also include an \"entropy\" bonus to the actor loss that encourages exploration\n",
        "\n",
        "\t\t\t\tcritic_gradient = torch.cat((critic_gradient, advantage*advantage))\n",
        "\n",
        "\t\t\t# update both models\n",
        "\t\t\tgradient = torch.cat((actor_gradient, critic_gradient))\n",
        "\t\t\tloss = update_net(gradient, optim)\n",
        "\t\t\tlosses.append(loss.detach())\n",
        "\n",
        "\treturn obj_func_hist, losses"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T13:05:15.283973Z",
          "iopub.status.idle": "2023-12-25T13:05:15.284510Z",
          "shell.execute_reply.started": "2023-12-25T13:05:15.284225Z",
          "shell.execute_reply": "2023-12-25T13:05:15.284251Z"
        },
        "trusted": true,
        "id": "xpA23AZ6FsNF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_fancy_loss(data, path):\n",
        "\n",
        "\tmeans, mins, maxes = list(map(list, zip(*data)))\n",
        "\n",
        "\tplt.clf()\n",
        "\n",
        "\txs = range(len(data))\n",
        "\n",
        "\t# Plot points\n",
        "\tplt.plot(xs, means, 'b', alpha=0.8)\n",
        "\t#plt.plot(pfb_x_mean, pfb_y_mean, 'g', alpha=0.8)\n",
        "\n",
        "\t# Plot errors\n",
        "\tplt.fill_between(xs, mins, maxes, color='b', alpha=0.3)\n",
        "\t#plt.fill_between(pfb_x_mean, pfb_y_low, pfb_y_high, color='g', alpha=0.3)\n",
        "\n",
        "\t# Set labels\n",
        "\tplt.title(f'Reward projection')\n",
        "\tplt.xlabel('Episode No.')\n",
        "\tplt.ylabel(f'Reward')\n",
        "\tplt.legend(['My PPO implementation'])#, 'PPO for Beginners'])\\\n",
        "\tplt.savefig(path)\n",
        "\n",
        "\n",
        "def plot_ugly_loss(data, length, name):\n",
        "\n",
        "\tplt.clf()\n",
        "\tplt.plot(list(range(length)), data)\n",
        "\tplt.savefig(f\"{base_path}/{name}loss.png\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T13:05:15.285939Z",
          "iopub.status.idle": "2023-12-25T13:05:15.286807Z",
          "shell.execute_reply.started": "2023-12-25T13:05:15.286497Z",
          "shell.execute_reply": "2023-12-25T13:05:15.286524Z"
        },
        "trusted": true,
        "id": "GOGzjKMeFsNI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lunar_lander_hyperparameters = {\n",
        "\t\"num_episodes\" : 300,\n",
        "\t\"gamma\" : 0.99,\n",
        "\t\"lr\" : 1e-3,\n",
        "\t\"env_name\" : \"LunarLander-v2\",\n",
        "\t\"render_mode\" : \"rgb_array\",\n",
        "\t\"trial_number\" : 5,\n",
        "\t\"eps\" : 0.2,\n",
        "\t\"num_epochs\" : 10,\n",
        "\t\"num_actors\" : 5,\n",
        "\t\"device\" : \"cuda\"\n",
        "}\n",
        "\n",
        "env = gym.make(lunar_lander_hyperparameters[\"env_name\"],\n",
        "\t\t\t   render_mode=lunar_lander_hyperparameters[\"render_mode\"],\n",
        "\t\t\t   continuous=False)\n",
        "\n",
        "base_path = f\"trial_data/trial_{lunar_lander_hyperparameters['trial_number']}\"\n",
        "\n",
        "env = gym.wrappers.RecordVideo(env, f\"{base_path}/video/\", episode_trigger=lambda t: t % 100 == 99)\n",
        "\n",
        "actor = Actor(lunar_lander_hyperparameters[\"device\"])\n",
        "actor.to(actor.device)\n",
        "\n",
        "critic = Critic(lunar_lander_hyperparameters[\"device\"])\n",
        "critic.to(critic.device)\n",
        "\n",
        "optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lunar_lander_hyperparameters[\"lr\"])\n",
        "\n",
        "obj_func_hist, losses = train(env,\n",
        "\t\t\t\t\t\t\t  actor,\n",
        "\t\t\t\t\t\t\t  critic,\n",
        "\t\t\t\t\t\t\t  optimizer,\n",
        "\t\t\t\t\t\t\t  lunar_lander_hyperparameters[\"num_episodes\"],\n",
        "\t\t\t\t\t\t\t  lunar_lander_hyperparameters[\"num_actors\"],\n",
        "\t\t\t\t\t\t\t  lunar_lander_hyperparameters[\"num_epochs\"],\n",
        "\t\t\t\t\t\t\t  lunar_lander_hyperparameters[\"eps\"],\n",
        "\t\t\t\t\t\t\t  lunar_lander_hyperparameters[\"gamma\"])\n",
        "\n",
        "\n",
        "with open(f'{base_path}/hyperparameters.txt', 'w') as f:\n",
        "    f.write(str(lunar_lander_hyperparameters))\n",
        "\n",
        "# check if directory exist, if not, make it\n",
        "Path(f\"{base_path}/save/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "plot_fancy_loss(obj_func_hist,\n",
        "\t\t\t\tf\"{base_path}/rewards.png\")\n",
        "\n",
        "plot_ugly_loss(losses,\n",
        "\t\t\t   len(losses),\n",
        "\t\t\t   \"total_\")\n",
        "\n",
        "torch.save(actor.state_dict(), f\"{base_path}/save/actor_weights.pt\")\n",
        "torch.save(critic.state_dict(), f\"{base_path}/save/critic_weights.pt\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T13:05:15.288936Z",
          "iopub.status.idle": "2023-12-25T13:05:15.289755Z",
          "shell.execute_reply.started": "2023-12-25T13:05:15.289440Z",
          "shell.execute_reply": "2023-12-25T13:05:15.289467Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XnmcmxHFsNK",
        "outputId": "072e6db3-128d-491d-8df4-8f919a29e3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 19/300 [02:50<38:13,  8.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/trial_data/trial_5/video/rl-video-episode-99.mp4.\n",
            "Moviepy - Writing video /content/trial_data/trial_5/video/rl-video-episode-99.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "t:   0%|          | 0/119 [00:00<?, ?it/s, now=None]\u001b[A\n",
            "t:   6%|▌         | 7/119 [00:00<00:01, 68.32it/s, now=None]\u001b[A\n",
            "t:  28%|██▊       | 33/119 [00:00<00:00, 179.57it/s, now=None]\u001b[A\n",
            "t:  44%|████▎     | 52/119 [00:00<00:00, 160.57it/s, now=None]\u001b[A\n",
            "t:  58%|█████▊    | 69/119 [00:00<00:00, 157.52it/s, now=None]\u001b[A\n",
            "t:  71%|███████▏  | 85/119 [00:00<00:00, 157.15it/s, now=None]\u001b[A\n",
            "t:  86%|████████▌ | 102/119 [00:00<00:00, 160.23it/s, now=None]\u001b[A\n",
            "t: 100%|██████████| 119/119 [00:00<00:00, 152.94it/s, now=None]\u001b[A\n",
            "  6%|▋         | 19/300 [02:51<38:13,  8.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/trial_data/trial_5/video/rl-video-episode-99.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█▏        | 34/300 [06:49<1:49:19, 24.66s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without gpu about 2hrs at 33/300\n",
        "# with gpu about at 1hr43m at 33/300"
      ],
      "metadata": {
        "id": "Bz8HIVH-GMjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}