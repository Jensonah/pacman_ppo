{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"14in29TlVQAd"}},{"cell_type":"code","source":"!pip install gymnasium[atari]\n!pip install gymnasium[accept-rom-license]\n!pip install moviepy","metadata":{"id":"y56tkQupVy9P","outputId":"695ef912-8a0e-4bc5-ab20-dc0ec0bcb90e","execution":{"iopub.status.busy":"2024-01-03T11:08:10.532328Z","iopub.execute_input":"2024-01-03T11:08:10.532601Z","iopub.status.idle":"2024-01-03T11:08:47.396178Z","shell.execute_reply.started":"2024-01-03T11:08:10.532577Z","shell.execute_reply":"2024-01-03T11:08:47.394629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gymnasium as gym\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport time\nimport json\nimport cv2","metadata":{"id":"Uh5YgfKjUEl0","execution":{"iopub.status.busy":"2024-01-03T11:08:47.402009Z","iopub.execute_input":"2024-01-03T11:08:47.402340Z","iopub.status.idle":"2024-01-03T11:08:49.648385Z","shell.execute_reply.started":"2024-01-03T11:08:47.402311Z","shell.execute_reply":"2024-01-03T11:08:49.647574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base.py","metadata":{"id":"fN8BFNBxVN0-"}},{"cell_type":"code","source":"class Actor(nn.Module):\n\n\tdef __init__(self, device, input_frame_dim, no_frames):\n\t\tsuper(Actor, self).__init__()\n\n\t\t# In this implementation the previous frames and the different rgb channels are in the same dimension\n\t\t# Reasoning being that on all channels the same kernels will work, and their wouldn't be a need to learn\n\t\t# different kernels for each frame\n\n\t\tself.conv1 = nn.Conv2d(3*no_frames, 3*no_frames, (5,5), groups=1, stride=(2,2))\n\t\t\n\t\tself.conv2 = nn.Conv2d(3*no_frames, 2*no_frames, (5,5), groups=1, stride=(1,1))\n\t\t\n\t\tself.conv3 = nn.Conv2d(2*no_frames, no_frames, (5,5), groups=1, stride=(1,1))\n\t\t\n\t\tself.conv4 = nn.Conv2d(no_frames, no_frames, (5,5), groups=1, stride=(1,1))\n\t\t\n\t\tself.conv5 = nn.Conv2d(no_frames, 1, (5,5), groups=1, stride=(1,1))\n\t\t\n\t\tself.flat = nn.Flatten()\n\t\t\n\t\tself.full1 = nn.Linear(35*22, 500)\n\n\t\tself.full2 = nn.Linear(500, 100)\n\t\t\n\t\tself.full3 = nn.Linear(100, 5)\n\n\t\tself.device = device\n\t\tself.input_frame_dim = input_frame_dim\n\t\tself.no_frames = no_frames\n\t\t\n\n\tdef forward(self, x):\n\n\t\tx = self.conv1(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.conv2(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.conv3(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.conv4(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.conv5(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.flat(x)\n\n\t\tx = self.full1(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.full2(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.full3(x)\t\t\n\t\tx = F.softmax(x, dim=1)\n\n\t\treturn x\n\t\n\n\tdef act(self, state_3):\n\n\t\t# we bring the last three frames to the model\n\t\t# that way the model can see how the ghosts move :)\n\t\t# Only the current frame should be attached\n\n\t\tprobabilities = self.forward(state_3)\n\t\tprobs = Categorical(probabilities)\n\t\taction = probs.sample()\n\t\t# PPO paper says to use normal prob here\n\t\treturn action.item(), probs.log_prob(action).exp()\n\t\n\nclass Critic(nn.Module):\n\n\tdef __init__(self, device, input_frame_dim, no_frames):\n\t\tsuper(Critic, self).__init__()\n\n\t\t# In this implementation the previous frames and the different rgb channels are in the same dimension\n\t\t# Reasoning being that on all channels the same kernels will work, and their wouldn't be a need to learn\n\t\t# different kernels for each frame\n\n\t\tself.conv1 = nn.Conv2d(3*no_frames, 3*no_frames, (5,5), groups=1, stride=(2,2))\n\t\t\n\t\tself.conv2 = nn.Conv2d(3*no_frames, 2*no_frames, (5,5), groups=1, stride=(1,1))\n\t\t\n\t\tself.conv3 = nn.Conv2d(2*no_frames, no_frames, (5,5), groups=1, stride=(1,1))\n\t\t\n\t\tself.conv4 = nn.Conv2d(no_frames, no_frames, (5,5), groups=1, stride=(1,1))\n\t\t\n\t\tself.conv5 = nn.Conv2d(no_frames, 1, (5,5), groups=1, stride=(1,1))\n\t\t\n\t\tself.flat = nn.Flatten()\n\n\t\tself.full1 = nn.Linear(35*22, 500)\n\n\t\tself.full2 = nn.Linear(500, 100)\n\t\t\n\t\tself.full3 = nn.Linear(100, 1)\n\n\t\tself.device = device\n\t\tself.input_frame_dim = input_frame_dim\n\t\tself.no_frames = no_frames\n\t\t\n\n\tdef forward(self, x):\n\n\t\tx = self.conv1(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.conv2(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.conv3(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.conv4(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.conv5(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.flat(x)\n\n\t\tx = self.full1(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.full2(x)\n\t\tx = F.relu(x)\n\n\t\tx = self.full3(x)\n\t\tx = F.sigmoid(x)\n\n\t\treturn x\n\n\ndef state_to_normalized_tensor(state, device, dim):\n\n\tstate = cv2.resize(state, (dim[2], dim[1]), interpolation=cv2.INTER_AREA)\n\n\tstate = np.array(state) / 255\n\n\t# PyTorch wants the rgb channel first\n\ttransposed_array = np.transpose(state, (2, 0, 1))\n\n\treturn torch.from_numpy(transposed_array).float().to(device)\n\n\ndef collect_episode(env, model):\n\n\tepisode = []\n\tterminated = False\n\ttruncated = False\n\n\t# (rgb, x, y) -> ((no_frames-1)*rgb, x, y)\n\tdim = model.input_frame_dim\n\tzeros_dim = (dim[0]*(model.no_frames-1), dim[1], dim[2])\n\n\tzeros = torch.from_numpy(np.zeros(zeros_dim))\n\t\n\tstate_repr = zeros.float().to(model.device)\n\n\tnew_state, info = env.reset()\n\n\tlast_life_value = info['lives']\n\t\n\twhile not (terminated or truncated):\n\n\t\tframe = state_to_normalized_tensor(new_state, model.device, dim)\n\t\tstate_repr = torch.cat((state_repr, frame))\n\t\tmodel_ready_state = state_repr.unsqueeze(0)\n\n\t\taction, probs = model.act(model_ready_state)\n\n\t\tnew_state, reward, terminated, truncated, info = env.step(action)\n\n\n\t\tif last_life_value > info['lives']:\n\t\t\tlast_life_value = info['lives']\n\t\t\t# in a previous implementation I always summed the number of current lives to the reward\n\t\t\t# this resulted in pacman hiding in a corner, as staying alive longer -> more rewards\n\t\t\t# now we just give a penalty when he dies\n\t\t\treward -= 100\n\n\t\t# For memory reasons we only save the frame within the state\n\t\t# These can be recovered later, using the functions provided\n\t\tepisode.append((frame, action, probs, reward))\n\n\t\t# 3 because we work with rgb, could also polished by integrating into hyperpars\n\t\t# we pop the last frame here\n\t\tstate_repr = state_repr[3:]\n\n\treturn episode","metadata":{"id":"Or-Y9X4GVHyu","outputId":"d5e84771-bf17-4492-fe38-a952b50b23e2","execution":{"iopub.status.busy":"2024-01-03T11:08:49.649905Z","iopub.execute_input":"2024-01-03T11:08:49.650428Z","iopub.status.idle":"2024-01-03T11:08:49.679998Z","shell.execute_reply.started":"2024-01-03T11:08:49.650390Z","shell.execute_reply":"2024-01-03T11:08:49.679161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train.py","metadata":{"id":"_QVI6THrVb_A"}},{"cell_type":"code","source":"# index probability of action taken at sampling time with current updated model\ndef get_probs(actor, frames, actions):\n\t#all_probs = torch.cat([actor.forward(state) for state in states])\n\tall_probs = torch.cat([actor.forward(get_state(i, frames, actor)) for i in range(len(frames))])\n\tactions = torch.Tensor(actions).int()\n\tout = all_probs[torch.arange(all_probs.size(0)), actions]\n\t# these lines do this below more efficient\n\t#[all_probs[i][actions[i]].unsqueeze(0) for i in range(len(actions))]\n\treturn out\n\n\ndef get_standardized_tensor(xs):\n\n\t## eps is the smallest representable float, which is \n\t# added to the standard deviation of the returns to avoid numerical instabilities   \n\teps = np.finfo(np.float32).eps.item()     \n\txs = torch.tensor(xs)\n\treturn (xs - xs.mean()) / (xs.std() + eps)\n\n\n# from the array of frames this function constructs the state at time step i\ndef get_state(i, frames, model):\n\n\tdevice = model.device\n\tno_frames = model.no_frames\n\tframe_dim = model.input_frame_dim\n\n\tno_empty_frames = no_frames - i - 1 # i = 0 & no_frames = 3 -> = 2 \n\tleft_idx = max(0, i - no_frames + 1)# i = 0 & no_frames = 3 -> = 0\n\tright_idx = i + 1\n\n\tframes_tensor = torch.cat((frames[left_idx:right_idx]))\n\n\tif no_empty_frames > 0:\n\t\tzeros = torch.zeros(3*no_empty_frames, frame_dim[1], frame_dim[2]).to(device)\n\t\tframes_tensor = torch.cat((zeros, frames_tensor))\n\t\n\treturn frames_tensor.unsqueeze(0)\n\n\n\n# TODO: make sure the amount of context frames can be a variable \n# (such that a state consists of the last x frames)\ndef train(env, actor, critic, optim, num_episodes, num_actors, num_epochs, eps, gamma):\n\n\t# One style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\n\t# with recurrent neural networks, runs the policy for T timesteps (where T is much less than the\n\t# episode length), and uses the collected samples for an update. This style requires an advantage\n\t# estimator that does not look beyond timestep T .\n\t# Don't we already have this? Think about this. Or isn't that the entire idea of the estimator?\n\n\tassert actor.device == critic.device\n\tassert actor.no_frames == critic.no_frames\n\tassert actor.input_frame_dim == critic.input_frame_dim\n\n\tdevice = actor.device\n\n\tobj_func_hist = []\n\tlosses = []\n\n\ttime_sampling = 0\n\ttime_updating = 0\n\n\tfor _ in tqdm(range(num_episodes)):\n\t\t\n\t\tt0 = time.time()\n\n\t\tepisodes = [collect_episode(env, actor) for _ in range(num_actors)]\n\n\n\t\t'''\n\t\tIn other implementations the data is not a collection of varying length concluded episodes,\n\t\tbut instead an array of i*j, where j is a fixed number of steps.\n\t\tSo a row could be a partial episode (most likely), or multiple entire episodes, \n\t\twith possibly the last one being truncated.\n\t\t'''\n\n\t\tframes = [[frame for frame, _, _, _ in episode] for episode in episodes]\n\t\tactions = [[action for _, action, _, _ in episode] for episode in episodes]\n\t\toriginal_probs  = [torch.cat([prob for _, _, prob, _ in episode]) for episode in episodes]\n\t\trewards = [[reward for _, _, _, reward in episode] for episode in episodes]\n\n\t\tcum_rewards = np.array([sum(episode) for episode in rewards])\n\t\tobj_func_hist.append((cum_rewards.mean(), cum_rewards.min(), cum_rewards.max()))\n\n\t\trewards = [get_standardized_tensor(reward).unsqueeze(1).to(device) for reward in rewards]\n\n\t\tt1 = time.time()\n\n\t\ttime_sampling += t1 - t0\n\n\t\tt0 = time.time()\n\t\tfor k in range(num_epochs):\n\n\t\t\tloss = 0\n\n\t\t\tfor j in range(num_actors):\n\t\t\t\t\n\t\t\t\t#states = [get_state(i, frames[j], no_frames, frame_dim) for i in range(len(frames[j]))]\n\n\t\t\t\t#critic_values_t  = torch.cat([critic(state) for state in states])\n\t\t\t\tcritic_values_t  = torch.cat([critic(get_state(i, frames[j], critic)) for i in range(len(frames[j]))])\n\t\t\t\tcritic_values_t1 = torch.cat((critic_values_t[1:], torch.zeros(1,1).to(device)))\n\n\t\t\t\tadvantage = gamma*critic_values_t1 + rewards[j] - critic_values_t\n\n\t\t\t\t# we need the probability for each action\n\t\t\t\tif k == 0:\n\t\t\t\t\tactor_probs = original_probs[j].clone()\n\t\t\t\t\toriginal_probs[j] = original_probs[j].detach()\n\t\t\t\telse:\n\t\t\t\t\t# we can also always do this and detach already all above\n\t\t\t\t\t#actor_probs = get_probs(actor, states, actions[j])\n\t\t\t\t\tactor_probs = get_probs(actor, frames[j], actions[j])\n\n\t\t\t\tdifference_grad = torch.div(actor_probs, original_probs[j]).unsqueeze(1)\n\t\t\t\t# Note that for k = 0 these are all ones, but we keep the calculation such that the\n\t\t\t\t# backtracking algorithm can also see this\n\t\t\t\t\n\t\t\t\tclipped = torch.clamp(difference_grad, 1 - eps, 1 + eps)\n\n\t\t\t\tppo_gradient = torch.minimum(difference_grad*advantage, clipped*advantage)\n\t\t\t\tppo_gradient *= -1 # this seems to be the right place\n\n\t\t\t\tloss += ppo_gradient.sum() + 0.5*((advantage**2).sum())\n\t\t\t\t# we could also include an \"entropy\" bonus to the actor loss that encourages exploration\n\n\t\t\t# update both models\n\t\t\toptim.zero_grad()\n\t\t\tloss.backward()\n\t\t\toptim.step()\n\t\t\tlosses.append(loss.detach().cpu())\n\t\t\n\t\tt1 = time.time()\n\n\t\ttime_updating += t1 - t0\n\n\tprint(f\"Total time sampling = {time_sampling}\")\n\tprint(f\"Total time updating = {time_updating}\")\n\n\treturn obj_func_hist, losses\n\n\ndef plot_fancy_loss(data, path):\n\n\tmeans, mins, maxes = list(map(list, zip(*data)))\n\n\tplt.clf()\n\n\txs = range(len(data))\n\n\t# Plot points\n\tplt.plot(xs, means, 'b', alpha=0.8)\n\t#plt.plot(pfb_x_mean, pfb_y_mean, 'g', alpha=0.8)\n\n\t# Plot errors\n\tplt.fill_between(xs, mins, maxes, color='b', alpha=0.3)\n\t#plt.fill_between(pfb_x_mean, pfb_y_low, pfb_y_high, color='g', alpha=0.3)\n\n\t# Set labels\n\tplt.title(f'Reward projection')\n\tplt.xlabel('Episode No.')\n\tplt.ylabel(f'Reward')\n\tplt.legend(['My PPO implementation'])#, 'PPO for Beginners'])\\\n\tplt.savefig(path)\n\n\ndef plot_ugly_loss(data, length, name):\n\n\tplt.clf()\n\tplt.plot(list(range(length)), data)\n\tplt.savefig(f\"{base_path}/{name}loss.png\")\n\n\n# 3 HOUR TRAINING TIME?????\npacman_hyperparameters = {\n\t\"num_episodes\" : 1000,\n\t\"gamma\" : 0.99,\n\t\"lr\" : 1e-3,\n\t\"env_name\" : \"ALE/MsPacman-v5\",\n\t\"frameskip\" : 4,\n\t\"repeat_action_probability\": 0.2,\n\t\"render_mode\" : \"rgb_array\",\n\t\"trial_number\" : 5,\n\t\"eps\" : 0.2,\n\t\"num_epochs\" : 6,\n\t\"num_actors\" : 3,\n\t\"device\" : \"cuda\",\n\t\"obs_type\" : \"rgb\",\n\t\"input_frame_dim\" : (3,210,160),\n\t\"no_frames\" : 2, # how many frames the model can look back (all frames given to model input)\n\t\"scale\" : 2, # how much we scale down the image in both x and y dimensions\n}\n\ndim = pacman_hyperparameters[\"input_frame_dim\"]\nscale = pacman_hyperparameters[\"scale\"]\ndim = (dim[0], dim[1]//scale, dim[2]//scale)\n\n# input (250, 160, 3)\n# reward float, increases when pacman eats. Further information about reward func unknown\n# info contains lives, dict object\n\nenv = gym.make(pacman_hyperparameters[\"env_name\"], \n\t\t\t   render_mode=pacman_hyperparameters[\"render_mode\"]\n\t\t\t   )\n\nbase_path = f\"trial_data/trial_{pacman_hyperparameters['trial_number']}\"\n\n# check if directories exist, if not, make it\nPath(f\"{base_path}\").mkdir(parents=True, exist_ok=True)\nPath(f\"{base_path}/video/\").mkdir(parents=True, exist_ok=True)\nPath(f\"{base_path}/save/\").mkdir(parents=True, exist_ok=True)\n\n#env = gym.wrappers.RecordVideo(env, f\"{base_path}/video/\", episode_trigger=lambda t: t % 100 == 2)\n\nactor = Actor(pacman_hyperparameters[\"device\"],\n\t\t\t  dim,\n\t\t\t  pacman_hyperparameters[\"no_frames\"])\nactor.to(actor.device)\n\ncritic = Critic(pacman_hyperparameters[\"device\"],\n\t\t\t\tdim,\n\t\t\t\tpacman_hyperparameters[\"no_frames\"])\ncritic.to(critic.device)\n\noptimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=pacman_hyperparameters[\"lr\"])\n\nobj_func_hist, losses = train(env,\n\t\t\t\t\t\t\t  actor,\n\t\t\t\t\t\t\t  critic,\n\t\t\t\t\t\t\t  optimizer,\n\t\t\t\t\t\t\t  pacman_hyperparameters[\"num_episodes\"],\n\t\t\t\t\t\t\t  pacman_hyperparameters[\"num_actors\"],\n\t\t\t\t\t\t\t  pacman_hyperparameters[\"num_epochs\"],\n\t\t\t\t\t\t\t  pacman_hyperparameters[\"eps\"],\n\t\t\t\t\t\t\t  pacman_hyperparameters[\"gamma\"])\n\njson.dump(pacman_hyperparameters, open(f\"{base_path}/hyperparameters.json\",'w'))\n\ntorch.save(actor.state_dict(), f\"{base_path}/save/actor_weights.pt\")\ntorch.save(critic.state_dict(), f\"{base_path}/save/critic_weights.pt\")\n\nplot_fancy_loss(obj_func_hist,\n\t\t\t\tf\"{base_path}/rewards.png\")\n\nplot_ugly_loss(losses,\n\t\t\t   len(losses),\n\t\t\t   \"total_\")\n\nenv.close()","metadata":{"id":"E9g5hGzGVa14","outputId":"7329ce09-c568-415f-9630-61ac2a47eab1","execution":{"iopub.status.busy":"2024-01-03T11:08:49.682512Z","iopub.execute_input":"2024-01-03T11:08:49.683071Z","iopub.status.idle":"2024-01-03T11:10:39.871952Z","shell.execute_reply.started":"2024-01-03T11:08:49.683044Z","shell.execute_reply":"2024-01-03T11:10:39.870978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}